# Geographic Hallucinations

This folder contains case studies where large language models (LLMs) confuse locations with similar names or make incorrect geographic inferences.

Each case includes:
- ✅ The original prompt
- ❌ The model’s (incorrect) output
- 📌 A brief explanation of the error and potential source of confusion
