# Geographic Hallucinations

This folder contains curated case studies where large language models (LLMs) confuse locations with similar names, conflate cultural or regional attributes, or make incorrect geographic inferences.

These errors often stem from:
- Ambiguity in the user prompt (e.g. "London", "Santiago")
- Majority-culture or Eurocentric bias
- Lack of grounding or contextual awareness
- Overgeneralization or flattening of regional distinctions
- Hallucination through narrative invention

Rather than focusing on “gotchas,” this collection aims to:
- Highlight common failure patterns across different models
- Compare how various LLMs (e.g. Gemini, LLaMA 2) handle geography-related queries
- Document subtle forms of misinformation, from poetic hallucinations to generic evasions

Each case includes:
- ✅ The original prompt
- ❌ The model’s (incorrect) output
- 📌 A brief explanation of the error and its possible source
- 🧭 Suggested mitigation strategies (when relevant)
- 🔎 Tags for categorization and future benchmarking



